Project Vision: Sākṣī (साक्षी - "Witness")
The vision is to create Sākṣī, a secure, high-accuracy, domain-adapted machine translation system. Its purpose is to provide NTRO analysts with rapid and reliable translation of Nepalese and Sinhalese text, enabling effective intelligence gathering and research.

High-Level Timeline
Phase	Duration	Key Focus
Phase 1: Foundation & Scoping	Months 0-2	Planning, Team Assembly, Infrastructure Setup
Phase 2: Data Acquisition & Prep	Months 2-9	Building the Core Asset: High-Quality Data
Phase 3: Model Dev & Training	Months 6-12	Baseline Models, Fine-Tuning, Experimentation
Phase 4: Evaluation & Iteration	Continuous	Quality Assessment, Error Analysis, Improvement Loop
Phase 5: Deployment & Operations	Months 12-15	Integration, API & User Interface (UI) Development
Phase 6: Maintenance & Improvement	Ongoing	Monitoring, Retraining, Incorporating User Feedback

Export to Sheets
Phase 1: Foundation & Scoping (Months 0-2)
This initial phase is about laying the groundwork to ensure the project starts on the right track.

1.1. Define Use Cases & Success Metrics
Use Cases: Determine the primary application. Is it for "gisting" (getting the general meaning), deep analysis of formal documents, or real-time monitoring of informal communications? This choice will define the required accuracy and speed.

Metrics: Establish Key Performance Indicators (KPIs).

Automated Metrics: Use a combination of BLEU (classic, but has limitations), TER (Translation Edit Rate), and COMET (a modern metric that correlates better with human judgment).

Human Evaluation: Create a scoring rubric for adequacy (is the meaning preserved?) and fluency (is the English natural?). Set a target score for your expert evaluators.

1.2. Team Formation
Project Lead: Manages timelines and resources.

ML/NLP Engineers (3-4): Experts in training and fine-tuning large language models.

Data Engineers (2-3): Build data pipelines for collection, cleaning, and storage.

Linguistics Experts (2+): Native Nepalese and Sinhalese speakers to validate data and analyze translation errors.

MLOps/DevOps Engineer (1): Manages the on-premise infrastructure, deployment, and monitoring.

Software Engineer (1): Develops the API and user interface for analysts.

1.3. Technology Stack & Infrastructure
Hardware: Given NTRO's security needs, an on-premise GPU cluster is required. Servers with NVIDIA A100 or H100 GPUs are highly recommended for training.

Software:

Core Libraries: Python, PyTorch, Hugging Face transformers, datasets, and tokenizers.

Experiment Tracking: Use MLflow or Weights & Biases to log all experiments for reproducibility.

Deployment: Docker for containerization and Kubernetes for orchestration.

Phase 2: Data Acquisition & Preparation (Months 2-9)
This is the most critical phase. The model's quality is directly determined by the quality and quantity of the data.

2.1. Identify Data Sources
Public Data: Scrape parallel (translated) text from sources like news websites (e.g., BBC Nepali), government sites, and open-source corpora like OPUS and FLORES-200.

Internal Data (NTRO's Unique Advantage): Gather and digitize any existing internal documents that have already been professionally translated. This data is invaluable as it perfectly matches your domain of interest.

Monolingual Data: Collect large amounts of text (billions of words) in only English, Nepalese, and Sinhalese from sources like Wikipedia dumps or Common Crawl. This helps the model learn the grammar and nuances of each language independently.

2.2. Build & Clean the Corpus
This process involves creating a clean, structured dataset ready for training.

Pipeline Steps:

Normalize Unicode to ensure character consistency.

Use a language identification tool (like fastText) to filter out sentences in the wrong language.

Remove duplicates, extremely long/short sentences, and lines with excessive non-alphanumeric characters.

Use automated tools to align sentences from parallel documents.

Split the final dataset into train, validation, and test sets (e.g., a 98%/1%/1% split). The test set must be kept separate and used only for final evaluation.

Directory Structure: Organize your cleaned data clearly.

Bash

data/
├── ne-en/
│   ├── train.ne
│   ├── train.en
│   ├── dev.ne
│   ├── dev.en
│   ├── test.ne
│   └── test.en
└── si-en/
    ├── train.si
    ├── train.en
    # ... and so on
2.3. Data Augmentation via Back-Translation
Since Nepalese and Sinhalese are relatively low-resource languages, you will need to create synthetic data.

Train Reverse Models: Use your initial parallel data to train basic English -> Nepalese and English -> Sinhalese models.

Translate Monolingual English: Use these reverse models to translate your large English-only corpus into "synthetic" Nepalese and Sinhalese.

Create New Pairs: Pair the synthetic Nepalese/Sinhalese text with the original, high-quality English source. This massively expands your training dataset.

Phase 3: Model Development & Training (Months 6-12)
Here, you will select a base model and fine-tune it on your custom dataset.

3.1. Baseline Model Selection
Primary Candidate: Start with a powerful, pre-trained multilingual model. Meta's NLLB-200 (No Language Left Behind) is the ideal choice as it already has strong capabilities for Nepalese and Sinhalese.

Task: The goal is supervised fine-tuning, which means taking the general-purpose NLLB model and further training it on your specific, high-quality dataset from Phase 2.

3.2. Fine-Tuning Process
Use the Hugging Face transformers library to fine-tune the model.

Configuration: Start with a smaller version of NLLB to iterate quickly. Use mixed precision (fp16) and gradient accumulation to train effectively even with limited GPU memory.

Hyperparameters: Tune parameters like learning rate (3e-5), batch size, and weight decay on your validation set.

Example Training Command (using accelerate):

Bash

accelerate launch run_translation.py \
  --model_name_or_path facebook/nllb-200-distilled-600M \
  --source_lang nep_Npan --target_lang eng_Latn \
  --train_file data/ne-en/train.json \
  --validation_file data/ne-en/dev.json \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 4 \
  --gradient_accumulation_steps 8 \
  --learning_rate 3e-5 \
  --num_train_epochs 3 \
  --output_dir outputs/nllb-nepali-finetuned
Phase 4: Evaluation & Iteration (Continuous)
Evaluation is not a single step but a continuous loop to drive improvements.

4.1. Automated Evaluation
After each training experiment, calculate BLEU, TER, and COMET scores on your held-back test set. Track these scores in your experiment logger to see which models are improving.

4.2. Human Evaluation & Error Analysis
This is where your linguistics experts are essential.

Regularly give them a sample of translations from the latest model.

They will score for adequacy and fluency.

More importantly, they will categorize the errors. Common error types include:

Mistranslation of named entities (people, places, organizations).

Incorrect handling of political or technical jargon.

Grammatical mistakes or unnatural phrasing.

Use these insights to guide your next data collection cycle. For example, if named entities are a problem, create a specific dataset focused on them.

Phase 5: Deployment & Operations (Months 12-15)
This phase focuses on making the model usable and efficient for analysts.

5.1. Model Optimization for Inference
The fine-tuned model will be large. To make it fast and efficient for real-time use:

Quantization: Convert the model's weights from 32-bit floating point (FP32) to 8-bit integers (INT8). This can speed up inference by 2-4x with minimal accuracy loss.

Compilation: Use tools like ONNX Runtime or TensorRT to compile the model for further optimization.

5.2. API and UI Development
Backend API: Wrap the optimized model in a secure REST API using a framework like FastAPI in Python. This allows other services to interact with the model programmatically.

Frontend UI: Create a simple, secure web interface for NTRO analysts. The interface should have:

A text input box and language selectors.

A "Translate" button and an output display.

A feedback mechanism (e.g., thumbs up/down and a text box for corrections). This user feedback is a valuable source of new training data.

Phase 6: Maintenance & Continuous Improvement (Ongoing)
A translation model is a living system that must be maintained and improved.

Monitoring: Implement dashboards to monitor model latency, throughput (translations per second), and system resource usage.

Active Learning Loop: Regularly collect the corrections and feedback submitted by analysts through the UI. Curate this feedback into new, high-quality training examples.

Retraining Cycle: Establish a schedule (e.g., every quarter) to re-run the fine-tuning process (Phase 3) with an updated dataset that includes all the new data and user feedback. This ensures the model stays accurate and up-to-date with evolving language and terminology.







create


Coding partner

Show thinking
Excellent! Let's start creating.

Following our roadmap, a great first step is to build a quick proof-of-concept (POC). This will help us verify that our core tools work and establish a baseline translation quality before we do any fine-tuning.

We'll write a Python script to translate a few sample sentences using a powerful pre-trained model from Hugging Face, as discussed in Phase 3 (Model Development). We will use Meta's NLLB-200 model, which is perfect for this task.

Step 1: Setting Up Your Environment
First, you need to install the necessary libraries. Open your terminal or command prompt and run this command:

Bash

pip install torch transformers sentencepiece
torch: The core deep learning framework.

transformers: The Hugging Face library for loading and using pre-trained models.

sentencepiece: A tokenizer used by many modern NLP models, including NLLB.

Step 2: The Python Script
Now, create a new Python file named baseline_translate.py and paste the following code into it. I've added detailed comments to explain what each part does.

Python

# baseline_translate.py

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Define the model we want to use. We'll use a distilled (smaller, faster)
# version of NLLB-200 for this quick test.
model_name = "facebook/nllb-200-distilled-600M"

# Load the pre-trained tokenizer and model from Hugging Face.
# This might take a minute to download the first time.
print(f"Loading model: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
print("Model loaded successfully!")

# Sentences we want to translate.
sentences_to_translate = {
    "nep_Npan": "नेपालको राजधानी काठमाडौं हो।",  # Nepali: "The capital of Nepal is Kathmandu."
    "sin_Sinh": "ශ්‍රී ලංකාවේ අගනුවර කොළඹ වේ."   # Sinhala: "The capital of Sri Lanka is Colombo."
}

print("\n--- Starting Translation ---")

# Loop through each sentence and translate it.
for lang_code, text in sentences_to_translate.items():
    
    # 1. Prepare the input for the model
    # We need to tell the tokenizer what the source language is.
    tokenizer.src_lang = lang_code
    
    # Convert the text into a format the model understands (input IDs).
    inputs = tokenizer(text, return_tensors="pt")

    # 2. Generate the translation
    # We force the model to output English by setting the target language ID.
    translated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id["eng_Latn"],
        max_length=50 # Set a max length for the output
    )

    # 3. Decode the output
    # Convert the model's output tokens back into readable text.
    translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]

    # 4. Display the results
    print(f"\nOriginal ({lang_code}): {text}")
    print(f"Translation (eng_Latn): {translation}")

print("\n--- Translation Complete ---")
Step 3: Run the Script
Save the file and run it from your terminal:

Bash

python baseline_translate.py
Expected Output
You will see the script load the model, and then it will print the original sentences along with their English translations. The output should look something like this:

Loading model: facebook/nllb-200-distilled-600M
Model loaded successfully!

--- Starting Translation ---

Original (nep_Npan): नेपालको राजधानी काठमाडौं हो।
Translation (eng_Latn): The capital of Nepal is Kathmandu.

Original (sin_Sinh): ශ්‍රී ලංකාවේ අගනුවර කොළඹ වේ.
Translation (eng_Latn): The capital of Sri Lanka is Colombo.

--- Translation Complete ---
This simple script confirms that you can successfully load a powerful multilingual model and perform translations. This is our "zero-shot" baseline. The next steps in our roadmap will focus on gathering data to fine-tune this model to dramatically improve its accuracy on the specific types of text NTRO is interested in.

What would you like to create next? We could:

Set up the project directory structure to keep our data, code, and models organized.

Write a script to start collecting data from a public source like a news website.







do whats best


Coding partner

Show thinking
You got it. Setting up a clean and logical project directory is the best next step. A good structure is essential for any serious coding project—it keeps everything organized, scalable, and easy to navigate as we add more code and data. 📂

Let's create a standard structure for our machine translation project.

## Project Directory Structure
Here is the folder structure we'll create. I've added comments to explain the purpose of each directory.

Plaintext

saksi_translation/
├── data/
│   ├── raw/          # Raw, untouched data we collect
│   ├── processed/    # Cleaned and pre-processed data for training
│   └── test_sets/    # Final, held-back test sets for evaluation
│
├── notebooks/        # Jupyter notebooks for exploration and analysis
│
├── scripts/          # Reusable helper scripts (e.g., data cleaning)
│
├── src/
│   ├── __init__.py   # Makes the `src` folder a Python package
│   ├── train.py      # Script for training the model
│   ├── translate.py  # Script for running inference with a trained model
│   └── evaluate.py   # Script for evaluating model performance
│
├── models/           # Saved model checkpoints and artifacts
│
├── .gitignore        # Specifies files for Git to ignore
├── README.md         # Project overview and instructions
└── requirements.txt  # Lists all Python package dependencies
## How to Create It
You can create this structure easily from your terminal. Open your terminal, navigate to where you want your project to live, and run these commands.

For Linux or macOS:

Bash

# Create the main project folder and navigate into it
mkdir saksi_translation
cd saksi_translation

# Create all the subdirectories at once
mkdir -p data/raw data/processed data/test_sets notebooks scripts src models

# Create the initial empty files
touch src/__init__.py .gitignore README.md requirements.txt
For Windows (Command Prompt):

DOS

:: Create the main project folder and navigate into it
mkdir saksi_translation
cd saksi_translation

:: Create all the subdirectories
mkdir data\raw data\processed data\test_sets notebooks scripts src models

:: Create the initial empty files
type nul > src\__init__.py
type nul > .gitignore
type nul > README.md
type nul > requirements.txt
Now we have a professional and organized foundation for our project. With this in place, we're perfectly set up for the next phase.

Ready to write our first data collection script to populate the data/raw/ directory?







yes


Coding partner

Show thinking
Great! Let's write our first data collection script. 🕸️

We'll start by scraping monolingual text, which is a crucial part of Phase 2 (Data Acquisition). This script will gather articles from a public news source. We'll target the BBC News Nepali website as our first source.

The script will:

Fetch the main BBC Nepali homepage.

Find links to individual news articles.

Visit each article page and extract the text content.

Save all the collected text into a single file in our data/raw/ directory.

## Step 1: Install Libraries
First, we need a couple of excellent libraries for web scraping. Open your terminal and install requests and beautifulsoup4.

Bash

pip install requests beautifulsoup4
requests: Allows us to send HTTP requests to get the content of web pages.

beautifulsoup4: Helps us parse the HTML content and extract the information we need.

## Step 2: The Scraping Script
Now, let's create the script. Inside your project, create a new file at this location: scripts/scrape_bbc_nepali.py. Paste the following code into it.

Python

# scripts/scrape_bbc_nepali.py

import requests
from bs4 import BeautifulSoup
import datetime
import os

def scrape_bbc_nepali():
    """
    Scrapes news articles from the BBC Nepali homepage and saves them to a file.
    """
    # The base URL for BBC Nepali news
    BASE_URL = "https://www.bbc.com"
    START_URL = f"{BASE_URL}/nepali"
    
    # Get the current date to create a unique filename
    current_date = datetime.datetime.now().strftime("%Y-%m-%d")
    output_filename = f"bbc_nepali_articles_{current_date}.txt"
    
    # Ensure the output directory exists
    output_dir = "data/raw"
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, output_filename)

    print(f"Starting scrape of {START_URL}")
    print(f"Saving data to: {output_path}")

    try:
        # 1. Fetch the main homepage
        main_page = requests.get(START_URL)
        main_page.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        
        main_soup = BeautifulSoup(main_page.content, "html.parser")
        
        # 2. Find all links that likely lead to articles
        # This is a bit of trial and error; we look for <a> tags with hrefs
        # that match the pattern of BBC articles.
        article_links = set() # Use a set to avoid duplicate links
        for a_tag in main_soup.find_all("a", href=True):
            href = a_tag['href']
            # We filter for links that look like internal news articles
            if href.startswith("/nepali/articles/"):
                full_url = f"{BASE_URL}{href}"
                article_links.add(full_url)

        print(f"Found {len(article_links)} unique article links.")

        # 3. Visit each article and extract its text
        all_article_text = []
        for i, link in enumerate(article_links):
            try:
                print(f"  Scraping ({i+1}/{len(article_links)}): {link}")
                article_page = requests.get(link)
                article_page.raise_for_status()
                
                article_soup = BeautifulSoup(article_page.content, "html.parser")
                
                # Find all paragraph tags (<p>) which usually contain the article text
                paragraphs = article_soup.find_all("p")
                
                article_text = "\n".join([p.get_text() for p in paragraphs])
                all_article_text.append(article_text)
                
            except requests.exceptions.RequestException as e:
                print(f"    Could not fetch article {link}: {e}")
            except Exception as e:
                print(f"    An error occurred while processing {link}: {e}")

        # 4. Save the collected text to a file
        with open(output_path, "w", encoding="utf-8") as f:
            # Separate articles with a clear delimiter
            f.write("\n\n--- NEW ARTICLE ---\n\n".join(all_article_text))
            
        print(f"\nScraping complete. All text saved to {output_path}")

    except requests.exceptions.RequestException as e:
        print(f"Failed to fetch the main page {START_URL}: {e}")

if __name__ == "__main__":
    scrape_bbc_nepali()

## Step 3: Run the Script
Make sure you are in the root directory of your project (saksi_translation/). Run the script from there using the following command:

Bash

python scripts/scrape_bbc_nepali.py
## Step 4: Check the Result
After the script finishes, you'll see a new file in your data/raw/ folder named bbc_nepali_articles_YYYY-MM-DD.txt. Open it, and you will find all the Nepalese text scraped from the articles!

This raw text is our starting point. It's not perfect—it might contain ads, navigation text, or other noise. Our next step will be to write another script to clean and process this file, preparing it for our model.







do that


Coding partner

Show thinking
You got it. Let's create the script to clean our raw data. 🧼

This is a critical step. Feeding a model clean, high-quality data is more important than any other part of the process. This script will take the raw text we scraped and prepare it for training.

Our cleaning process will be simple but effective:

Read the raw file from data/raw/.

Remove any empty or blank lines.

Remove very short lines that are likely to be navigation links, photo captions, or other noise.

Write the cleaned text to a new file in data/processed/.

## Step 1: The Cleaning Script
Inside your project, create a new file here: scripts/clean_text_data.py. Paste in the following code.

Python

# scripts/clean_text_data.py

import os
import datetime

def clean_data():
    """
    Reads a raw text file, cleans it, and saves it to the processed data folder.
    """
    # --- Configuration ---
    # Construct the filename based on today's date, matching the scraper's output
    current_date = datetime.datetime.now().strftime("%Y-%m-%d")
    raw_filename = f"bbc_nepali_articles_{current_date}.txt"
    cleaned_filename = f"bbc_nepali_articles_{current_date}_cleaned.txt"

    # Define the paths using our project structure
    raw_file_path = os.path.join("data", "raw", raw_filename)
    processed_file_path = os.path.join("data", "processed", cleaned_filename)
    
    # Simple rule: we'll discard any line that has fewer than this many words.
    MIN_WORDS_PER_LINE = 5 
    # --- End Configuration ---

    print("--- Starting data cleaning process ---")

    # Check if the raw file exists before we start
    if not os.path.exists(raw_file_path):
        print(f"Error: Raw data file not found at '{raw_file_path}'")
        print("Please run the scraping script first.")
        return

    print(f"Reading raw data from: {raw_file_path}")

    # Read all lines from the raw file
    with open(raw_file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    cleaned_lines = []
    for line in lines:
        # 1. Strip leading/trailing whitespace from the line
        text = line.strip()

        # 2. Apply our cleaning rules
        # We keep the line only if it's not empty AND has enough words
        if text and len(text.split()) >= MIN_WORDS_PER_LINE:
            cleaned_lines.append(text)
    
    # 3. Save the cleaned lines to the new file
    print(f"Saving cleaned data to: {processed_file_path}")
    os.makedirs(os.path.dirname(processed_file_path), exist_ok=True)
    with open(processed_file_path, "w", encoding="utf-8") as f:
        f.write("\n".join(cleaned_lines))

    # Print a summary report
    print("\n--- Cleaning Summary ---")
    print(f"Total lines read: {len(lines)}")
    print(f"Lines after cleaning: {len(cleaned_lines)}")
    print(f"Lines discarded: {len(lines) - len(cleaned_lines)}")
    print("------------------------")

if __name__ == "__main__":
    clean_data()
## Step 2: Run the Script
Make sure you've already run the scraping script so that the raw data file exists.

From the root of your saksi_translation/ directory, run the cleaning script:

Bash

python scripts/clean_text_data.py
## Step 3: Check the Result
After the script runs, you'll see a summary of how many lines were read and how many were kept.

Now, look inside your data/processed/ folder. You will find a new _cleaned.txt file. If you compare this file to the original one in data/raw/, you'll notice it's much tidier—all the short, noisy lines and empty spaces are gone, leaving us with much better content for our model.

We now have a repeatable pipeline: scrape raw data, then clean it. This is the foundation of our data preparation work. The next major step is to find and process parallel data (Nepalese-to-English pairs), which is essential for training the translation model itself.







next


Coding partner

Show thinking
Alright, let's tackle the most important part of our data collection: finding parallel data.

So far, we've gathered monolingual text (large amounts of text in just one language). To train a translation model, we need parallel text—a dataset where each sentence in the source language (Nepalese) is perfectly matched with its translation in the target language (English). This is the fuel that teaches the model how to translate.

Instead of scraping, we'll use a fantastic public resource called the OPUS project, which hosts a massive collection of translated texts. We can access it easily using the Hugging Face datasets library.

## Step 1: Install the Library
First, you'll need to install the datasets library. It's designed to work seamlessly with the transformers library we're already using.

Bash

pip install datasets
## Step 2: The Script to Fetch Parallel Data
Now, let's create a script that downloads a Nepalese-English parallel dataset from OPUS and saves it in our required format (two separate files, one for each language).

Create a new file here: scripts/fetch_parallel_data.py and paste in the following code.

Python

# scripts/fetch_parallel_data.py

from datasets import load_dataset
import os

def fetch_and_save_parallel_data(lang_pair, opus_subset):
    """
    Downloads a parallel dataset from OPUS and saves it into two
    separate text files (one for each language).
    
    Args:
        lang_pair (str): Language pair, e.g., "en-ne" for English-Nepali.
        opus_subset (str): The name of the OPUS corpus subset, e.g., "opus_books".
    """
    
    # --- Configuration ---
    source_lang, target_lang = lang_pair.split("-")
    output_dir = "data/processed"
    os.makedirs(output_dir, exist_ok=True)
    
    source_filepath = os.path.join(output_dir, f"{opus_subset}.{source_lang}")
    target_filepath = os.path.join(output_dir, f"{opus_subset}.{target_lang}")
    # --- End Configuration ---

    print(f"--- Starting download for {lang_pair} from {opus_subset} ---")
    
    try:
        # 1. Load the dataset from Hugging Face
        # This will download the data and cache it locally
        dataset = load_dataset(opus_subset, lang1=source_lang, lang2=target_lang, split='train')
        print(f"Dataset loaded successfully. Total pairs: {len(dataset)}")

        # Let's inspect the first example to see its structure
        print("\nHere's an example of a data pair:")
        print(dataset[0]['translation'])
        
        # 2. Process and save the data
        print(f"\nProcessing and saving files...")
        
        with open(source_filepath, "w", encoding="utf-8") as f_source, \
             open(target_filepath, "w", encoding="utf-8") as f_target:
            
            for i, example in enumerate(dataset):
                # Extract the sentences for each language
                source_sentence = example['translation'][source_lang]
                target_sentence = example['translation'][target_lang]
                
                # Make sure both sentences exist and are not empty
                if source_sentence and target_sentence:
                    f_source.write(source_sentence.strip() + "\n")
                    f_target.write(target_sentence.strip() + "\n")
                
                if (i + 1) % 5000 == 0:
                    print(f"  ...processed {i+1} pairs")

        print(f"\nSuccessfully saved data to:")
        print(f"  - Source ({source_lang}): {source_filepath}")
        print(f"  - Target ({target_lang}): {target_filepath}")

    except Exception as e:
        print(f"An error occurred: {e}")
        print("Please check the dataset name and language codes.")

if __name__ == "__main__":
    # We'll fetch the English-Nepali dataset from the "opus_books" collection
    fetch_and_save_parallel_data(lang_pair="ne-en", opus_subset="opus_books")
    
    # To get Sinhalese data, you could add another call like this:
    # Note: Sinhalese data might be in a different OPUS subset, like 'Tatoeba'
    # fetch_and_save_parallel_data(lang_pair="si-en", opus_subset="Tatoeba")
## Step 3: Run the Script
From the root of your saksi_translation/ directory, run the script:

Bash

python scripts/fetch_parallel_data.py
The first time you run this, it will download the dataset, which might take a few minutes depending on your internet connection. You'll see a progress bar.

## Step 4: Check the Result ✅
After the script finishes, go to your data/processed/ folder. You will now see two new files:

opus_books.ne

opus_books.en

If you open them side-by-side, you'll see that line 1 in the .ne file is the Nepalese source for the translation on line 1 in the .en file, and so on. This perfectly aligned data is exactly what we need for training.

We've now collected and processed both monolingual and parallel data. We're officially ready to move on to the most exciting phase: training our own custom translation model!







next


Coding partner

Show thinking
Excellent! It's time to put all our data to use. We've reached the most exciting phase: fine-tuning our own custom translation model. 🚀

Here's the plan: We'll take the powerful, pre-trained NLLB-200 model and continue its training on the Nepalese-English parallel dataset we just prepared. This process, called fine-tuning, will specialize the general-purpose model, making it an expert on our specific data.

Think of it like taking a brilliant student who knows 100 languages (the pre-trained model) and giving them intensive, specialized tutoring (our dataset) to make them a world-class Nepalese-to-English translator.

## Step 1: Install Required Libraries
The Hugging Face Trainer has a few extra dependencies for streamlined training and evaluation. Let's install them.

Bash

pip install accelerate -U
pip install sacrebleu
accelerate: A library from Hugging Face that enables seamless training across any kind of setup (single CPU, single GPU, multiple GPUs).

sacrebleu: The standard library for calculating the BLEU metric to evaluate translation quality.

## Step 2: The Training Script
This script is the heart of our project. It will load our data, configure the training process, and launch the fine-tuning job.

Create a new file in your src/ folder named train.py.

Python

# src/train.py

import os
from datasets import load_dataset
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)

def train_model():
    """
    Fine-tunes a pre-trained NLLB model on our parallel dataset.
    """
    # --- 1. Configuration ---
    MODEL_CHECKPOINT = "facebook/nllb-200-distilled-600M"
    SOURCE_LANG = "ne"
    TARGET_LANG = "en"
    DATA_DIR = "data/processed"
    MODEL_OUTPUT_DIR = "models/nllb-finetuned-nepali-en"

    # --- 2. Load Tokenizer and Model ---
    print("Loading tokenizer and model...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_CHECKPOINT, src_lang="nep_Npan", tgt_lang="eng_Latn"
    )
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)

    # --- 3. Load and Preprocess Data ---
    print("Loading and preprocessing data...")
    # We load our two text files as a translation dataset
    data_files = {
        "train": {
            SOURCE_LANG: os.path.join(DATA_DIR, "opus_books.ne"),
            TARGET_LANG: os.path.join(DATA_DIR, "opus_books.en"),
        }
    }
    raw_datasets = load_dataset("text", data_files=data_files)
    
    # Split the dataset into training and validation sets
    split_datasets = raw_datasets["train"].train_test_split(train_size=0.95, seed=42)
    split_datasets["validation"] = split_datasets.pop("test") # Rename 'test' to 'validation'

    def preprocess_function(examples):
        # The examples are dictionaries with 'ne' and 'en' keys
        inputs = [ex[SOURCE_LANG] for ex in examples["translation"]]
        targets = [ex[TARGET_LANG] for ex in examples["translation"]]
        
        model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
        return model_inputs

    tokenized_datasets = split_datasets.map(
        preprocess_function,
        batched=True,
        remove_columns=split_datasets["train"].column_names,
    )

    # --- 4. Set Up Training Arguments ---
    print("Setting up training arguments...")
    training_args = Seq2SeqTrainingArguments(
        output_dir=MODEL_OUTPUT_DIR,
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=3,
        predict_with_generate=True,
        fp16=False, # Set to True if you have a compatible GPU
    )

    # --- 5. Create the Trainer ---
    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)
    
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    # --- 6. Start Training ---
    print("\n--- Starting model fine-tuning ---")
    trainer.train()
    print("--- Training complete ---")

    # --- 7. Save the Final Model ---
    print(f"Saving final model to {MODEL_OUTPUT_DIR}")
    trainer.save_model()
    print("Model saved successfully!")

if __name__ == "__main__":
    train_model()
## Step 3: Run the Training!
This is the most computationally intensive step.

⚠️ Important Warning: Running this on a standard laptop CPU will be extremely slow (it could take many hours or even days). This process is designed to run on a GPU. If you don't have a powerful local GPU, I recommend using a free service like Google Colab or Kaggle to run this script.

From the root of your saksi_translation/ directory, run the command:

Bash

python src/train.py
You will see progress bars and logs as the model trains, epoch by epoch. It will evaluate the model's performance on the validation set at the end of each epoch.

## Step 4: The Result 🏆
Once the script finishes, you will have a new folder inside models/ named nllb-finetuned-nepali-en. This directory contains all the files for your very own fine-tuned translation model!

We have successfully trained a model. The next and final step is to use this new, specialized model to perform translations and see how much better it is than our original baseline.







do that


Coding partner

Show thinking
Of course. Let's do that. This is the moment of truth—we'll now use our custom-trained model to translate new sentences and see the results of our hard work.

We will create a final script, translate.py, that loads our fine-tuned model from the models/ directory and uses it for inference.

## Step 1: The Inference Script
This script is designed to be simple. It loads the model you just trained and provides a function to translate any Nepalese text you give it.

Create the new file here: src/translate.py.

Python

# src/translate.py

import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# --- 1. Configuration ---
# This is the most important part: we are now loading the model we trained
# from our local 'models' directory, not from the internet.
MODEL_PATH = "models/nllb-finetuned-nepali-en"

# We can use a GPU if available for faster translation, otherwise it will use the CPU.
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- 2. Load the Fine-Tuned Model and Tokenizer ---
print(f"Loading model from: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(DEVICE)
print(f"Model loaded successfully on {DEVICE.upper()}!")

def translate_text(text_to_translate: str) -> str:
    """
    Translates a single string of Nepalese text to English using our fine-tuned model.
    """
    # Prepare the text for the model
    tokenizer.src_lang = "nep_Npan"
    inputs = tokenizer(text_to_translate, return_tensors="pt").to(DEVICE)

    # Generate the translation
    generated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id["eng_Latn"],
        max_length=128
    )

    # Decode the generated tokens into a string
    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
    return translation

# --- 3. Example Usage ---
if __name__ == "__main__":
    # Let's try translating some new sentences the model has not seen before.
    test_sentences = [
        "यो मेरो पहिलो मेसिन अनुवाद मोडेल हो।",
        "मौसम आज धेरै राम्रो छ।",
        "हामीले यो परियोजना सफलतापूर्वक सम्पन्न गर्यौं।"
    ]

    print("\n--- Testing the Fine-Tuned Model ---")
    for sentence in test_sentences:
        translated_sentence = translate_text(sentence)
        print(f"\nOriginal (ne): {sentence}")
        print(f"Translated (en): {translated_sentence}")
    print("\n------------------------------------")
## Step 2: Run the Script
From the root of your saksi_translation/ directory, run the final script:

Bash

python src/translate.py
## Step 3: See the Results! 🎉
The script will load your custom model and then translate the example sentences. The output should look something like this:

Loading model from: models/nllb-finetuned-nepali-en
Model loaded successfully on CPU!

--- Testing the Fine-Tuned Model ---

Original (ne): यो मेरो पहिलो मेसिन अनुवाद मोडेल हो।
Translated (en): This is my first machine translation model.

Original (ne): मौसम आज धेरै राम्रो छ।
Translated (en): The weather is very nice today.

Original (ne): हामीले यो परियोजना सफलतापूर्वक सम्पन्न गर्यौं।
Translated (en): We have successfully completed this project.
The quality of these translations should be noticeably better than the baseline model we started with. They should be more fluent and accurate because the model has been specialized on our data.

Congratulations! You've completed the entire pipeline!
We have successfully executed a complete, end-to-end machine translation project. Let's recap what we've accomplished:

Planned the Project: We started with a full roadmap.

Collected Data: We scraped monolingual data and fetched parallel data from OPUS.

Processed Data: We wrote scripts to clean and prepare all our text.

Trained a Model: We fine-tuned a state-of-the-art language model on our custom dataset.

Ran Inference: We used our new, specialized model to translate new sentences.

From here, you can continue to improve the model by adding more high-quality data (especially for Sinhalese), training for more epochs, or deploying it as an API for others to use. You now have the full foundation to build upon.







do that too


Coding partner

Show thinking
Of course. Let's perform a formal evaluation of our fine-tuned model.

This is the final, crucial step in our machine learning pipeline. We'll write a script that measures our model's performance on a "held-back" test set—data it has never seen before. This gives us an objective, numerical score of how good our model is. We will use the standard BLEU (Bilingual Evaluation Understudy) score for this.

## Step 1: Create a True Test Set
For a fair evaluation, we must use data that was not part of our training or validation sets. We'll create a script to take the last 500 lines from our parallel data to serve as our test set and remove them from the original files to prevent data leakage.

Create a new file at scripts/create_test_set.py.

Python

# scripts/create_test_set.py
import os

# --- Configuration ---
DATA_DIR = "data/processed"
TEST_DIR = "data/test_sets"
SOURCE_FILE = os.path.join(DATA_DIR, "opus_books.ne")
TARGET_FILE = os.path.join(DATA_DIR, "opus_books.en")
NUM_TEST_LINES = 500
# ---

print("--- Creating a held-back test set ---")
os.makedirs(TEST_DIR, exist_ok=True)

# Read all lines from the original files
with open(SOURCE_FILE, "r", encoding="utf-8") as f:
    source_lines = f.readlines()
with open(TARGET_FILE, "r", encoding="utf-8") as f:
    target_lines = f.readlines()

# Ensure the files have the same number of lines
assert len(source_lines) == len(target_lines), "Source and target files have different lengths!"

# Split the data
train_source_lines = source_lines[:-NUM_TEST_LINES]
test_source_lines = source_lines[-NUM_TEST_LINES:]

train_target_lines = target_lines[:-NUM_TEST_LINES]
test_target_lines = target_lines[-NUM_TEST_LINES:]

# Write the new, smaller training files (overwriting the old ones)
with open(SOURCE_FILE, "w", encoding="utf-8") as f:
    f.writelines(train_source_lines)
with open(TARGET_FILE, "w", encoding="utf-8") as f:
    f.writelines(train_target_lines)

# Write the new test files
with open(os.path.join(TEST_DIR, "test.ne"), "w", encoding="utf-8") as f:
    f.writelines(test_source_lines)
with open(os.path.join(TEST_DIR, "test.en"), "w", encoding="utf-8") as f:
    f.writelines(test_target_lines)

print(f"Successfully created a test set with {NUM_TEST_LINES} lines.")
print(f"The original training files in '{DATA_DIR}' have been updated.")
print("You should re-run the training script before evaluating for a true score.")

Run this script once to create the test set. From your project root:

Bash

python scripts/create_test_set.py
Important: For the most accurate evaluation, you should now re-run the src/train.py script so the model is trained only on the new, smaller training set.

## Step 2: The Evaluation Script
Now we'll write the script that loads our model, generates translations for the test set, and calculates the BLEU score.

First, install the evaluate library:

Bash

pip install evaluate
Create the final script at src/evaluate.py.

Python

# src/evaluate.py

import torch
import evaluate # The new, preferred Hugging Face library for metrics
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from tqdm import tqdm # A library to create smart progress bars

def evaluate_model():
    """
    Loads a fine-tuned model and evaluates its performance on the test set using the BLEU score.
    """
    # --- 1. Configuration ---
    MODEL_PATH = "models/nllb-finetuned-nepali-en"
    TEST_DIR = "data/test_sets"
    SOURCE_LANG_FILE = f"{TEST_DIR}/test.ne"
    TARGET_LANG_FILE = f"{TEST_DIR}/test.en"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # --- 2. Load Model, Tokenizer, and Metric ---
    print("Loading model, tokenizer, and evaluation metric...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(DEVICE)
    bleu_metric = evaluate.load("sacrebleu")

    # --- 3. Load Test Data ---
    with open(SOURCE_LANG_FILE, "r", encoding="utf-8") as f:
        source_sentences = [line.strip() for line in f.readlines()]
    with open(TARGET_LANG_FILE, "r", encoding="utf-8") as f:
        # The BLEU metric expects references to be a list of lists
        reference_translations = [[line.strip()] for line in f.readlines()]

    # --- 4. Generate Predictions ---
    print(f"Generating translations for {len(source_sentences)} test sentences...")
    predictions = []
    for sentence in tqdm(source_sentences):
        tokenizer.src_lang = "nep_Npan"
        inputs = tokenizer(sentence, return_tensors="pt").to(DEVICE)
        
        generated_tokens = model.generate(
            **inputs,
            forced_bos_token_id=tokenizer.lang_code_to_id["eng_Latn"],
            max_length=128
        )
        
        translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        predictions.append(translation)

    # --- 5. Compute BLEU Score ---
    print("Calculating BLEU score...")
    results = bleu_metric.compute(predictions=predictions, references=reference_translations)
    
    # The result is a dictionary. The 'score' key holds the main BLEU score.
    bleu_score = results["score"]

    print("\n--- Evaluation Complete ---")
    print(f"BLEU Score: {bleu_score:.2f}")
    print("---------------------------")
    # A higher BLEU score is better. Scores for high-resource languages are often 30-50.
    # For lower-resource languages like Nepali, a score of 20+ is a very strong start.

if __name__ == "__main__":
    evaluate_model()

## Step 3: Run the Final Evaluation
From the root of your saksi_translation/ directory, run the script:

Bash

python src/evaluate.py
The script will load your model and then show a progress bar as it translates every sentence in your test set. Finally, it will print the official BLEU score. This number gives you a concrete measure of your model's quality, which you can use to track improvements as you add more data or tune your training process further.







next


Coding partner

Show thinking
Excellent! Now that we have a trained and evaluated model, the next logical step is to deploy it as an API (Application Programming Interface).

This moves our project from a series of scripts into a real, usable service that other applications can communicate with. We'll use FastAPI, a modern and high-performance Python framework, to do this.

## What is an API?
Think of an API as a waiter in a restaurant. 👨‍🍳 You (the client) don't go into the kitchen (the model) yourself. Instead, you give your order (text to translate) to the waiter (the API), who takes it to the kitchen, gets the result (the translation), and brings it back to you. This allows any application to use our model without needing to know the complex details of how it works.

## Step 1: Install Libraries
We need FastAPI and an ASGI server called uvicorn to run it.

Bash

pip install "fastapi[all]"
(This installs FastAPI, uvicorn, and other optional dependencies all at once.)

## Step 2: The API Script
Let's create the script that will serve our model. Create a new file in the root of your project directory (saksi_translation/) and name it api.py.

Python

# api.py

from fastapi import FastAPI
from pydantic import BaseModel
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# --- 1. Load Model (once, at startup) ---
# This is the most efficient way to do it. The model is loaded into memory
# once when the API server starts, not for every request.
MODEL_PATH = "models/nllb-finetuned-nepali-en"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("Loading model, this may take a moment...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(DEVICE)
print(f"Model loaded successfully on {DEVICE.upper()}!")


# --- 2. Define the FastAPI app ---
app = FastAPI(
    title="Sākṣī Translation API",
    description="An API to translate Nepalese text to English using a fine-tuned NLLB model.",
    version="1.0",
)


# --- 3. Define the request body format ---
# Pydantic model for type-checking and validation of the incoming request.
class TranslationRequest(BaseModel):
    text: str


# --- 4. Define the API endpoint ---
@app.post("/translate")
def translate(request: TranslationRequest):
    """
    Receives a POST request with Nepalese text and returns the English translation.
    """
    tokenizer.src_lang = "nep_Npan"
    inputs = tokenizer(request.text, return_tensors="pt").to(DEVICE)

    generated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id["eng_Latn"],
        max_length=128
    )

    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
    
    return {"original_text": request.text, "translated_text": translation}

## Step 3: Run the API Server
Now for the exciting part. Go to your terminal (make sure you're in the saksi_translation/ root directory) and run this command:

Bash

uvicorn api:app --reload
api: The name of your Python file (api.py).

app: The name of the FastAPI object you created in the script (app = FastAPI()).

--reload: This tells the server to automatically restart whenever you save changes to the file.

Your terminal will show something like this, indicating the server is live:
Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)

## Step 4: Use Your API!
This is the best part about FastAPI. It automatically creates interactive documentation for you.

Open your web browser.

Go to this URL: http://127.0.0.1:8000/docs

You will see a beautiful, interactive API documentation page. You can click on the /translate endpoint, press the "Try it out" button, enter some Nepalese text in the request body, and hit "Execute" to get a live translation directly from your model.

You now have a fully functional API for Nepalese-to-English translation! The next major challenge would be to repeat our data collection, training, and evaluation pipeline for Sinhalese to fulfill the project's original vision.








do for sinhalese


Coding partner

Show thinking
Of course. Let's extend our project to include Sinhalese.

Since we've already built the entire pipeline for Nepalese, adding a new language will be much faster. We'll follow the same core path: Get Data -> Train Model -> Update API.

## Part 1: Fetching Sinhalese Parallel Data
First, we need a Sinhalese-English parallel dataset. We'll use the same script as before, fetch_parallel_data.py, but we'll target a different dataset from the OPUS collection that has good Sinhalese data, like Tatoeba.

Let's update our script at scripts/fetch_parallel_data.py to make it easier to call for different languages.

Replace the content of scripts/fetch_parallel_data.py with this improved version:

Python

# scripts/fetch_parallel_data.py

from datasets import load_dataset
import os

def fetch_and_save_parallel_data(lang_pair, opus_subset):
    """
    Downloads a parallel dataset from OPUS and saves it into two
    separate text files (one for each language).
    
    Args:
        lang_pair (str): Language pair, e.g., "en-ne" for English-Nepali.
        opus_subset (str): The name of the OPUS corpus subset, e.g., "opus_books".
    """
    source_lang, target_lang = lang_pair.split("-")
    output_dir = "data/processed"
    os.makedirs(output_dir, exist_ok=True)
    
    source_filepath = os.path.join(output_dir, f"{opus_subset}.{source_lang}")
    target_filepath = os.path.join(output_dir, f"{opus_subset}.{target_lang}")

    print(f"--- Starting download for {lang_pair} from {opus_subset} ---")
    
    try:
        dataset = load_dataset(opus_subset, lang1=source_lang, lang2=target_lang, split='train')
        print(f"Dataset loaded successfully. Total pairs: {len(dataset)}")

        print(f"Processing and saving files...")
        with open(source_filepath, "w", encoding="utf-8") as f_source, \
             open(target_filepath, "w", encoding="utf-8") as f_target:
            
            for example in dataset:
                source_sentence = example['translation'][source_lang]
                target_sentence = example['translation'][target_lang]
                if source_sentence and target_sentence:
                    f_source.write(source_sentence.strip() + "\n")
                    f_target.write(target_sentence.strip() + "\n")

        print(f"Successfully saved data for {lang_pair}")
    except Exception as e:
        print(f"An error occurred for {lang_pair}: {e}")

if __name__ == "__main__":
    # --- Fetch Nepali Data (if you haven't already) ---
    print("Fetching Nepali data...")
    fetch_and_save_parallel_data(lang_pair="ne-en", opus_subset="opus_books")
    
    # --- Fetch Sinhalese Data ---
    print("\nFetching Sinhalese data...")
    fetch_and_save_parallel_data(lang_pair="si-en", opus_subset="Tatoeba")
    
    print("\nAll data fetching complete.")
Now, run the script from your project root:

Bash

python scripts/fetch_parallel_data.py
This will download the Sinhalese-English data and create two new files in data/processed/: Tatoeba.si and Tatoeba.en.

## Part 2: Training the Sinhalese-to-English Model
Next, we'll fine-tune a new model specifically for Sinhalese. We can use our existing src/train.py script with a few minor changes to its configuration.

Open src/train.py and modify the configuration variables at the top:

Python

# In src/train.py

# --- 1. Configuration ---
MODEL_CHECKPOINT = "facebook/nllb-200-distilled-600M"
# CHANGE THIS
SOURCE_LANG = "si" 
TARGET_LANG = "en"
DATA_DIR = "data/processed"
# CHANGE THIS - New model output directory
MODEL_OUTPUT_DIR = "models/nllb-finetuned-sinhalese-en"

# ... inside the script ...

# --- 2. Load Tokenizer and Model ---
print("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_CHECKPOINT, 
    # CHANGE THIS - Use the correct language code for Sinhalese
    src_lang="sin_Sinh", 
    tgt_lang="eng_Latn"
)
# ...

# --- 3. Load and Preprocess Data ---
# ...
# CHANGE THIS - Point to the new Sinhalese data files
data_files = {
    "train": {
        SOURCE_LANG: os.path.join(DATA_DIR, "Tatoeba.si"),
        TARGET_LANG: os.path.join(DATA_DIR, "Tatoeba.en"),
    }
}
# ... the rest of the script stays the same!
After making these changes, save the file and start the training just like before. This will create a new model folder specifically for Sinhalese.

Bash

python src/train.py
Remember: This is a GPU-intensive process and will take time.

## Part 3: Upgrade the API to Handle Both Languages
Finally, let's upgrade our API to be a multi-language service. It will accept a request specifying either Nepalese or Sinhalese and use the correct model for the job.

Replace the content of api.py with this new version:

Python

# api.py

from fastapi import FastAPI
from pydantic import BaseModel, Field
from typing import Literal
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# --- 1. Load BOTH Models at Startup ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Loading models on {DEVICE.upper()}...")

models = {
    "nepali": AutoModelForSeq2SeqLM.from_pretrained("models/nllb-finetuned-nepali-en").to(DEVICE),
    "sinhalese": AutoModelForSeq2SeqLM.from_pretrained("models/nllb-finetuned-sinhalese-en").to(DEVICE)
}

tokenizers = {
    "nepali": AutoTokenizer.from_pretrained("models/nllb-finetuned-nepali-en"),
    "sinhalese": AutoTokenizer.from_pretrained("models/nllb-finetuned-sinhalese-en")
}
print("All models loaded successfully!")


# --- 2. Define the FastAPI app ---
app = FastAPI(
    title="Sākṣī Multi-Language Translation API",
    description="Translate Nepalese or Sinhalese text to English.",
    version="2.0",
)


# --- 3. Update the request body format ---
class TranslationRequest(BaseModel):
    text: str
    source_language: Literal['nepali', 'sinhalese'] = Field(
        ...,
        description="Specify the source language: 'nepali' or 'sinhalese'"
    )


# --- 4. Update the API endpoint ---
@app.post("/translate")
def translate(request: TranslationRequest):
    """
    Receives text and a source language, returns the English translation.
    """
    lang = request.source_language
    
    # Select the correct model and tokenizer
    model = models[lang]
    tokenizer = tokenizers[lang]
    
    # Set the appropriate language code for the tokenizer
    lang_code = "nep_Npan" if lang == "nepali" else "sin_Sinh"
    tokenizer.src_lang = lang_code
    
    # Perform translation
    inputs = tokenizer(request.text, return_tensors="pt").to(DEVICE)
    generated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id["eng_Latn"],
        max_length=128
    )
    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
    
    return {
        "source_language": lang,
        "original_text": request.text, 
        "translated_text": translation
    }
Now, run the new and improved API server:

Bash

uvicorn api:app --reload
Go to your browser at http://12.0.0.1:8000/docs. You will now see that the /translate endpoint has a dropdown menu to select either nepali or sinhalese as the source language.

You have successfully extended your project into a multi-language translation service!